{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Unit tests and coverage\n",
    "\n",
    "- [1. Introduction](#1.-Introduction)\n",
    "- [2. Coverage](#2.-Coverage)\n",
    "    - [2.1 Statement coverage](#2.1-Statement-coverage)\n",
    "    - [2.2 Branch coverage](#2.2-Branch-coverage)\n",
    "    - [2.3 Dataflow coverage](#2.3-Dataflow-coverage)\n",
    "- [3. More unit tests](#3.-More-unit-tests)\n",
    "- [4. Mocking](#4.-Mocking)\n",
    "- [5. Coverage revisited](#5.-Coverage-revisited)\n",
    "- [BONUS: `doctest`](#BONUS:-doctest)\n",
    "- [6. Submit to Canvas](#6.-Submit-to-Canvas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "For a new self-driving car, we need an implementation of a high-precision pi: ChatGPT v4 suggests the following implementation for computing pi in Python, including a unit test. The code is packed in the two files `estimate_pi.py` and `test_estimate_pi.py`. \n",
    "\n",
    "Run the existing test using your shell (every cell starting with an `!` will be executed in your OS's shell). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".E\n",
      "======================================================================\n",
      "ERROR: test_file_writer (__main__.TestPiWriter)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jakob/SoftwareTesting/Assignment1/test_estimate_pi.py\", line 14, in test_file_writer\n",
      "    with unittest.mock.patch('__main__.__builtins__.open', unittest.mock.mock_open()) as mocked_file:\n",
      "  File \"/usr/lib/python3.10/unittest/__init__.py\", line 95, in __getattr__\n",
      "    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n",
      "AttributeError: module 'unittest' has no attribute 'mock'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.392s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    }
   ],
   "source": [
    "!python3 test_estimate_pi.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is problematic with that test ChatGPT created for us and would you address this problem?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems:\n",
    "- You should import the mock module instead of writing unittest.mock.mock_upen. The better approach would be to write import unittest.mock and then replace ```unittest.mock.mock_open()``` with ```unittest.mock.Mock()```. This in my opinion makes the code easier to read and follow, but not neccisarily more efficient.\n",
    "- The fake file path variable should not be static like it is, for instance the filepath may change and by using a module like OS might solve the problem of filepaths. Since in its current state its dependant of the correct directory.\n",
    "- Regarding the TestPiWriter the classmethod **test_file_writer** is using a mocked version of the built in function **open** to simulate an open file. Sure, the write functionallity is tested however i would add functionality to test the readability of the file aswell to ensure that its not being corrupted or something like that. I would also add more test cases for invalid filepaths, invalid charaters in filepaths or of the content written is too long.\n",
    "- Another thing i would add since its checking if the write function and open is called once, i would also check so that the file is properly closed since we are just **mocking** an open file. So what i would add is ```mocked_file().close.assert_called_once()```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coverage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Statement coverage\n",
    "Compute the statement coverage of the program using [`coverage.py`](https://coverage.readthedocs.io/en/latest/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                  Stmts   Miss  Cover   Missing\n",
      "---------------------------------------------------\n",
      "estimate_pi.py           25     11    56%   18-19, 23-35\n",
      "test_estimate_pi.py      17      4    76%   19-21, 25\n",
      "---------------------------------------------------\n",
      "TOTAL                    42     15    64%\n"
     ]
    }
   ],
   "source": [
    "!coverage report -m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we interprete the results?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way i think of this result is that in the **test_estimate_pi.py file**, we cover 76% of the code from **estimate_pi.py**. Meaning we test 76 % of it. The missing column tells us what we are not testing, which makes sense if we look at the code. Line 18-19 is where we open the file in **estimate_pi.py** since we are mocking in the **test_estimate_pi.py** we do not really open any files, and i assume thats what it means. Then it also says that we are missing testcases for line 23-35, however thats nothing we really need to test, so i will disregard that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Branch coverage\n",
    "Now compute the statement coverage of the program using [`coverage.py`](https://coverage.readthedocs.io/en/latest/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we interprete the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dataflow coverage\n",
    "\n",
    "Draw the flow graph for the function `estimate_pi` defined in `estimate_pi.py`. Annotate the graph with definition and use information. Note: Please submit a separate image file or PDF with the name `dataflow_coverage.<file_extension>` for this task.\n",
    "\n",
    "Identify and describe the minimum number of test cases to achieve: all-defs coverage, and all-uses coverage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. More unit tests\n",
    "\n",
    "Add two more unit tests with the principles you learned in the lecture. Describe what principle you have used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mocking\n",
    "\n",
    "We want to store the resulting number persistently on our file system. We use the following class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiFileWriter:\n",
    "    @staticmethod\n",
    "    def write(content, file_path):\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a test double for `PiFileWriter` and add your implementation to `test_estimate_pi.py`. Discuss what type of test double you have implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name three other types of unit tests you would want to mock and explain why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Coverage revisited\n",
    "\n",
    "Rerun statement and branch coverage and discuss the differences and changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: `doctest`\n",
    "\n",
    "If you are curious or want to stand out, check out [`doctest`](https://en.wikipedia.org/wiki/Doctest). This task is optional. \n",
    "\n",
    "Add two `doctest` test cases and run the `doctest` tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you like `doctest`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Submit to Canvas\n",
    "\n",
    "Almost done, but the most tricky part is missing: submitting. :)\n",
    "\n",
    "Before submitting, make sure\n",
    "- you completed all non-optional tasks in this assignment (i.e., all empty cells are filled with meaningful content)\n",
    "- you don't use external libraries except `coverage.py`\n",
    "- the notebook runs straight through\n",
    "- your test code works\n",
    "- your code is readable and follows the Python coding conventions\n",
    "\n",
    "All set? Great. Just two steps away from happiness. \n",
    "\n",
    "1. Go through the list above and check again\n",
    "2. Submit *three* files to canvas:\n",
    "    - `assignment.ipynb`\n",
    "    - `test_estimate_pi.py`\n",
    "    - `dataflow_coverage.<file_extension>`\n",
    "3. Take a deep breath and carpe diem.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
